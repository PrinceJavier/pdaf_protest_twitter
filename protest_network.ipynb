{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Spread of PDAF Protest Sentiment on Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T08:44:53.068106Z",
     "start_time": "2019-03-08T08:44:53.065696Z"
    }
   },
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:54.412Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import ast\n",
    "import re\n",
    "\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "from collections import Counter \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:54.610Z"
    }
   },
   "outputs": [],
   "source": [
    "# load data paths\n",
    "paths = glob.glob(\"data/*\")\n",
    "paths_tweets_to_merge = [path for path in paths if \"tweets\" in path and \"merged\" not in path]\n",
    "paths_tweets_to_merge.remove(\"data/tweets_mpm_list_following.json\")\n",
    "paths_tweets_to_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:54.617Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge tweets dataset\n",
    "\n",
    "hashtags = [\"millionpeoplemarch\", \"janetnapoles\", \"onepinoy\", \"pdafkalampag\", \"porkbarrel\",\n",
    "            \"porkbarrelscam\", \"protestangbayan\", \"pdafscam\", \"scrappork\", \"stoppork\",\n",
    "            \"tayoangboss\", \"yesconchitacan\"]\n",
    "\n",
    "df_tweets_merged = pd.DataFrame()\n",
    "\n",
    "for path, hashtag in zip(paths_tweets_to_merge, hashtags):\n",
    "\n",
    "    df = pd.read_json(path)\n",
    "    # add a column for the #hashtag\n",
    "    df[\"hashtag\"] = hashtag\n",
    "    \n",
    "    # merge\n",
    "    df_tweets_merged = pd.concat([df_tweets_merged, df])\n",
    "    \n",
    "# reset index\n",
    "df_tweets_merged = df_tweets_merged.reset_index(drop=True)\n",
    "\n",
    "# filter only those tweets in 2013\n",
    "# still get NaT values\n",
    "condition_1 = df_tweets_merged.date.isnull()\n",
    "# get time after June 2013\n",
    "condition_2 = df_tweets_merged.date > pd.Timestamp('2013-06-01 00:00:00')\n",
    "# get time before Jan 2014\n",
    "condition_3 = df_tweets_merged.date < pd.Timestamp('2014-01-01 00:00:00')\n",
    "\n",
    "# merge conditions\n",
    "merged_conditions = condition_1 | condition_2 & condition_3\n",
    "df_tweets_merged = df_tweets_merged[merged_conditions]\n",
    "df_tweets_merged = df_tweets_merged.sort_values(by=\"date\").reset_index(drop=True)\n",
    "\n",
    "# # save to json\n",
    "pd.DataFrame.to_json(df_tweets_merged, \"data/tweets_merged.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:54.623Z"
    }
   },
   "outputs": [],
   "source": [
    "## IMPLEMENT KEY DUPLICATE REMOVER DURING CLEANING?\n",
    "## Dictionaries automatically remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:54.629Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_dict_from_file(path):\n",
    "\n",
    "    # open scraped file as string\n",
    "    f = open(path).read()\n",
    "\n",
    "    # convert scraped file from string to dictionary\n",
    "    dic = ast.literal_eval(f)\n",
    "\n",
    "    # return dictionary of file\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:54.635Z"
    }
   },
   "outputs": [],
   "source": [
    "# REMOVE BELOW NON-Number/Alphabet characters\n",
    "# DO THE SAME FOR THE TWITTER PANDAS DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:54.642Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_dict(dirty_dict, name_type=\"screen_name\"):\n",
    "\n",
    "    # define new dictionary\n",
    "    new_dict = {}\n",
    "\n",
    "    # get keys from input dirty dict\n",
    "    keys = dirty_dict.keys()\n",
    "\n",
    "    for key in keys:\n",
    "\n",
    "        # format is [{\"full_name\" : \"XX\", \"screen_name\" : \"YY\"}]\n",
    "        value = dirty_dict[key]\n",
    "        \n",
    "        # pattern for cleaning\n",
    "        pattern = re.compile(\"[\\w]+\")\n",
    "\n",
    "        # clean full_names and make lower_case\n",
    "        # get followers full_name per user\n",
    "        full_names = [\"\".join(re.findall(pattern, d[name_type])).lower() for d in value]\n",
    "\n",
    "#         # get followers screen name per user\n",
    "#         screen_names = [d[\"screen_name\"] for d in value]\n",
    "    \n",
    "        # clean keys\n",
    "        new_key = \"\".join(re.findall(pattern, key)).lower()\n",
    "        \n",
    "        # make key: value pair using full_name per user\n",
    "        new_dict[new_key] = full_names\n",
    "\n",
    "    # return new_dict\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Follower and Following Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:54.902Z"
    }
   },
   "outputs": [],
   "source": [
    "# load dirty followers dictionary\n",
    "path = \"data/user_followers.json\"\n",
    "dirty_followers = read_dict_from_file(path)\n",
    "\n",
    "# load dirty following dictionary\n",
    "path = \"data/user_following.json\"\n",
    "dirty_following = read_dict_from_file(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:54.908Z"
    }
   },
   "outputs": [],
   "source": [
    "# get clean followers, full_name\n",
    "clean_followers = clean_dict(dirty_followers, \"screen_name\")\n",
    "\n",
    "# get clean following, full_name\n",
    "clean_following = clean_dict(dirty_following, \"screen_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:54.914Z"
    }
   },
   "outputs": [],
   "source": [
    "# function to combine (get union) dictionaries\n",
    "def union_dict(a, b):\n",
    "    # a is the bigger dataset we will update with b\n",
    "    for k, v in a.items():\n",
    "        # if key is in a and b, then just append to current data in a\n",
    "        if k in b.keys():\n",
    "            # get union of unique items (remove redundancy)\n",
    "            a[k] = list(set(a[k] + b[k]))\n",
    "\n",
    "    # append keys in b that are not in a\n",
    "    for k, v in b.items():\n",
    "        if k not in a.keys():\n",
    "            a[k] = v\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:54.920Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "a = {\"a\":[1, 2, 5], \"b\":[2, 3, 4]}\n",
    "b = {\"a\":[1, 2, 3, 4], \"c\":[8, 9]}\n",
    "union_dict(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:54.926Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert following to followers format\n",
    "# e.g. {\"user_A\": [\"followed_user_1\", \"followed_user_2\"]\n",
    "# turn to: {\"followed_user_1\" : [\"user_A], \"followed_user_2\": [\"user_A\"]}\n",
    "# So network direction goes from user (followed) to follower\n",
    "\n",
    "def swap_key_val(old_dic):\n",
    "\n",
    "    # get keys of dictionary to swap\n",
    "    keys = old_dic.keys()\n",
    "\n",
    "    # convert keys to vals and vals to keys\n",
    "    # initiate new dictioanary (swapped)\n",
    "    new_dic = {}\n",
    "    for key in keys:\n",
    "        # swap value and key\n",
    "        a = {val: [key] for val in old_dic[key]}\n",
    "        # add to new_dic using union_dict function\n",
    "        new_dic = union_dict(new_dic, a)\n",
    "    return new_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:54.933Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "a = {\"follower_a\":[\"a\", \"b\"], \"follower_b\":[\"b\", \"c\", \"d\"]}\n",
    "b = swap_key_val(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:54.938Z"
    }
   },
   "outputs": [],
   "source": [
    "a = clean_followers.copy()\n",
    "b = clean_following.copy()\n",
    "\n",
    "# get followers data from following data\n",
    "followers_from_following = swap_key_val(b)\n",
    "\n",
    "# get union of followers data (scraped) and followers from following data\n",
    "unified_followers = union_dict(a, followers_from_following)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:54.947Z"
    }
   },
   "outputs": [],
   "source": [
    "# save unified_followers to json\n",
    "with open('data/unified_followers.json', 'w') as f:\n",
    "    json.dump(unified_followers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:54.953Z"
    }
   },
   "outputs": [],
   "source": [
    "# load saved file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:54.958Z"
    }
   },
   "outputs": [],
   "source": [
    "# get unified following data\n",
    "# get following data from followers data\n",
    "\n",
    "a = clean_followers.copy()\n",
    "b = clean_following.copy()\n",
    "\n",
    "following_from_followers = swap_key_val(a)\n",
    "\n",
    "# get union of followers data (scraped) and followers from following data\n",
    "unified_following = union_dict(b, following_from_followers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:54.965Z"
    }
   },
   "outputs": [],
   "source": [
    "# load saved file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:54.972Z"
    }
   },
   "outputs": [],
   "source": [
    "# save unified_following to json\n",
    "with open('data/unified_following.json', 'w') as f:\n",
    "    json.dump(unified_following, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:54.980Z"
    }
   },
   "outputs": [],
   "source": [
    "# length of scraped following\n",
    "len(clean_following)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:54.986Z"
    }
   },
   "outputs": [],
   "source": [
    "# length of unified following\n",
    "len(unified_following)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:54.991Z"
    }
   },
   "outputs": [],
   "source": [
    "# num of all users:followers scraped\n",
    "len(unified_followers.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:54.997Z"
    }
   },
   "outputs": [],
   "source": [
    "# num in just clean_followers\n",
    "len(clean_followers.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:55.003Z"
    }
   },
   "outputs": [],
   "source": [
    "# num in user:followers from following data\n",
    "len(followers_from_following.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Followers Graph - Asymmetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a graph of all the nodes in the followers and following dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:55.308Z"
    }
   },
   "outputs": [],
   "source": [
    "# # load Graph of followers\n",
    "# G = nx.from_dict_of_lists(unified_followers, create_using=nx.DiGraph())\n",
    "# len(G.nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T16:19:51.475809Z",
     "start_time": "2019-03-07T16:19:51.472826Z"
    }
   },
   "source": [
    "### Basic Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:55.447Z"
    }
   },
   "outputs": [],
   "source": [
    "# # out_degrees\n",
    "# degs = [k for n, k in G.out_degree]\n",
    "# avg_deg = np.mean(degs)\n",
    "# min_deg = np.min(degs)\n",
    "# max_deg = np.max(degs)\n",
    "\n",
    "# vals = {\"avg_degree\":[avg_deg], \"min_degree\":[min_deg], \"max_degree\":[max_deg]}\n",
    "# pd.DataFrame.from_dict(vals).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:55.455Z"
    }
   },
   "outputs": [],
   "source": [
    "# # in_degrees\n",
    "# degs = [k for n, k in G.in_degree]\n",
    "# avg_deg = np.mean(degs)\n",
    "# min_deg = np.min(degs)\n",
    "# max_deg = np.max(degs)\n",
    "\n",
    "# vals = {\"avg_degree\":[avg_deg], \"min_degree\":[min_deg], \"max_degree\":[max_deg]}\n",
    "# pd.DataFrame.from_dict(vals).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:55.462Z"
    }
   },
   "outputs": [],
   "source": [
    "# # degrees\n",
    "# degs = [k for n, k in G.degree]\n",
    "# avg_deg = np.mean(degs)\n",
    "# min_deg = np.min(degs)\n",
    "# max_deg = np.max(degs)\n",
    "\n",
    "# vals = {\"avg_degree\":[avg_deg], \"min_degree\":[min_deg], \"max_degree\":[max_deg]}\n",
    "# pd.DataFrame.from_dict(vals).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:55.467Z"
    }
   },
   "outputs": [],
   "source": [
    "# clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:55.589Z"
    }
   },
   "outputs": [],
   "source": [
    "# # out degree distribution\n",
    "# degs = [v for k, v in G.out_degree]\n",
    "\n",
    "# # count\n",
    "# deg_count = Counter(degs)\n",
    "\n",
    "# # plot\n",
    "# plt.loglog(deg_count.keys(), deg_count.values(), 'o', alpha=0.5)\n",
    "\n",
    "# plt.xlabel(\"out_degree\")\n",
    "# plt.ylabel(\"num nodes\")\n",
    "# plt.title(\"Out degree distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:55.594Z"
    }
   },
   "outputs": [],
   "source": [
    "# # in degree distribution\n",
    "# degs = [v for k, v in G.in_degree]\n",
    "\n",
    "# # count\n",
    "# deg_count = Counter(degs)\n",
    "\n",
    "# # plot\n",
    "# plt.loglog(deg_count.keys(), deg_count.values(), 'o', alpha=0.5)\n",
    "\n",
    "# plt.xlabel(\"in_degree\")\n",
    "# plt.ylabel(\"num nodes\")\n",
    "# plt.title(\"In degree distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:55.599Z"
    }
   },
   "outputs": [],
   "source": [
    "# # degree distribution\n",
    "# degs = [v for k, v in G.degree]\n",
    "\n",
    "# # count\n",
    "# deg_count = Counter(degs)\n",
    "\n",
    "# # plot\n",
    "# plt.loglog(deg_count.keys(), deg_count.values(), 'o', alpha=0.5)\n",
    "\n",
    "# plt.xlabel(\"degree\")\n",
    "# plt.ylabel(\"num nodes\")\n",
    "# plt.title(\"Degree distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:55.605Z"
    }
   },
   "outputs": [],
   "source": [
    "# VISUALIZATION\n",
    "# # Use spring layout\n",
    "# pos = nx.spring_layout(G)\n",
    "\n",
    "# # draw graph\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# nx.draw(G, pos=pos, node_size=2, with_labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Followers Subgraph - Symmetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below only selects that nodes that mutually follow each other (regardless if they tweeted about the protest or not). a <--> b will be preserved while c --> d will not be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:55.890Z"
    }
   },
   "outputs": [],
   "source": [
    "# def filter_symmetric(matrix, nodes):\n",
    "#     # get only symmetric\n",
    "#     matrix_sym = np.multiply(matrix, matrix.T)\n",
    "    \n",
    "#     # make diagonal = 0\n",
    "#     np.fill_diagonal(matrix_sym, 0)\n",
    "    \n",
    "#     # get indices with at least one degree in the symmetric network\n",
    "#     nonzero = np.array(np.sum(matrix_sym, axis=0))\n",
    "#     inds = list(np.nonzero(nonzero[0])[0])\n",
    "    \n",
    "\n",
    "\n",
    "#     # get nodes corresponding to index of nodes in symm network\n",
    "#     sym_nodes = np.array(nodes)[inds]\n",
    "    \n",
    "#     return matrix_sym, sym_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:55.897Z"
    }
   },
   "outputs": [],
   "source": [
    "# # TEST\n",
    "# matrix = np.matrix([[1, 0, 1, 1], [1, 1, 0, 1], [1, 0, 0, 0], [0, 1, 1, 1]])\n",
    "# nodes = ['a', 'b', 'c', 'd']\n",
    "# filter_symmetric(matrix, nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:55.902Z"
    }
   },
   "outputs": [],
   "source": [
    "# # make adjacency matrix\n",
    "# matrix = nx.adjacency_matrix(G).todense()\n",
    "\n",
    "# # get nodes list\n",
    "# nodes = list(G.nodes)\n",
    "\n",
    "# # THIS SHOULD BE EQUAL\n",
    "# matrix.shape, len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:55.908Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # CAN WE REMOVE THIS? SINCE THE IMPLEMENTATION BELOW WORKS AND THIS BREAKS\n",
    "# # THIS KILLS THE KERNEL\n",
    "# # filter symmetric adjacencty matrix\n",
    "# sym_matrix, sym_nodes = filter_symmetric(matrix, nodes)\n",
    "\n",
    "# print(\"No. of links in asymmetric network:\", np.sum(matrix))\n",
    "# print(\"Shape of asymmetric network:\", matrix.shape)\n",
    "# print(\"No. of links in symmetric network:\", np.sum(sym_matrix)/2)\n",
    "# print(\"Shape of symmetric network:\", sym_matrix.shape, len(sym_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:55.914Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filter symmetric given dictionary of followers only\n",
    "def filter_symmetric_from_dict(dic):\n",
    "    # get dictionary of followers\n",
    "    \n",
    "    # define new dictionary    \n",
    "    sym_dic = {}\n",
    "    \n",
    "    # run through keys and values\n",
    "    for k, v in dic.items():\n",
    "        # check if symmetric by looking at key and valu\n",
    "        for i in v:\n",
    "            try:\n",
    "                if k in dic[i]:\n",
    "                    if k in sym_dic:\n",
    "                        sym_dic[k] += [i]\n",
    "                    else:\n",
    "                        sym_dic[k] = [i]\n",
    "            except:\n",
    "                continue\n",
    "    return sym_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:55.919Z"
    }
   },
   "outputs": [],
   "source": [
    "a = {'z':['e', 'b'], 'a':['b', 'c', 'd'], 'b':['a', 'z'], 'c':['a'], 'd':['c'], 'e':['z']}\n",
    "filter_symmetric_from_dict(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:55.924Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Filter the network for symmetric only \n",
    "\n",
    "# # list of users in symmetric network\n",
    "# user_list = list(unified_followers.keys())\n",
    "\n",
    "# # filter only users in symmetric network\n",
    "# symm_followers = filter_symmetric_from_dict(unified_followers)\n",
    "\n",
    "# # make graph\n",
    "# H = nx.from_dict_of_lists(symm_followers, create_using=nx.DiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:55.930Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Number of nodes in symmetric network\n",
    "# len(H.nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T16:19:51.475809Z",
     "start_time": "2019-03-07T16:19:51.472826Z"
    }
   },
   "source": [
    "### Basic Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:56.077Z"
    }
   },
   "outputs": [],
   "source": [
    "# degs = [k for n, k in H.degree]\n",
    "# avg_deg = np.mean(degs)\n",
    "# min_deg = np.min(degs)\n",
    "# max_deg = np.max(degs)\n",
    "\n",
    "# vals = {\"avg_degree\":[avg_deg], \"min_degree\":[min_deg], \"max_degree\":[max_deg]}\n",
    "# pd.DataFrame.from_dict(vals).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:56.467Z"
    }
   },
   "outputs": [],
   "source": [
    "# # degree distribution\n",
    "# degs = [v for k, v in H.degree]\n",
    "\n",
    "# # count\n",
    "# deg_count = Counter(degs)\n",
    "\n",
    "# # plot\n",
    "# plt.loglog(deg_count.keys(), deg_count.values(), 'o', alpha=0.5)\n",
    "\n",
    "# plt.xlabel(\"degree\")\n",
    "# plt.ylabel(\"num nodes\")\n",
    "# plt.title(\"Degree distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:56.474Z"
    }
   },
   "outputs": [],
   "source": [
    "# VISUALIZATION\n",
    "# # Use spring layout\n",
    "# pos = nx.spring_layout(H)\n",
    "\n",
    "# # draw graph\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# nx.draw(H, pos=pos, node_size=2, with_labels=True, font_color='b', font_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fraction of Recruited Users Over Time (Nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:56.630Z"
    }
   },
   "outputs": [],
   "source": [
    "## CONSIDER RETWEETS - CURRENTLY CODE BELOW REMOVES RETWEETS DUE TO LACK OF DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:56.635Z"
    }
   },
   "outputs": [],
   "source": [
    "# loading Tweets as dataframe\n",
    "tweets_0 = pd.read_json(\"data/tweets_merged.json\")\n",
    "\n",
    "# remove hashtag column to avoid duplicates (some tweets have >1 hashtags)\n",
    "tweets_0 = tweets_0.drop(\"hashtag\", axis=1)\n",
    "\n",
    "# drop duplicates by tweet_id EXCEPT NULL -- HOW TO DO THIS?\n",
    "tweets_0 = tweets_0[tweets_0['tweet_id'].isnull(\n",
    ") | ~tweets_0[tweets_0['tweet_id'].notnull()].duplicated(subset='tweet_id', keep='first')]\n",
    "\n",
    "# cleaning up user_screen_name\n",
    "pattern = re.compile(\"[\\w]+\")\n",
    "tweets_0.user_screen_name = tweets_0.user_screen_name.apply(\n",
    "    lambda x: re.findall(pattern, \"\".join(re.findall(pattern, x)).lower())[0])\n",
    "\n",
    "# convert date to date time\n",
    "tweets = tweets_0.copy()\n",
    "tweets.date = pd.to_datetime(tweets_0.date)\n",
    "\n",
    "# remove date outside 2013\n",
    "tweets = tweets[(tweets['date'] > '2013-01-01 00:00:00')\n",
    "                & (tweets['date'] <= '2013-12-31 00:00:00')]\n",
    "\n",
    "# HOW ABOUT THE RETWEETS\n",
    "\n",
    "# sort values by date\n",
    "tweets = tweets.sort_values(by=\"date\")\n",
    "\n",
    "# drop duplicates in case there are duplicates\n",
    "tweets = tweets.drop_duplicates(keep=\"first\")\n",
    "\n",
    "# reset index\n",
    "tweets = tweets.reset_index()\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:56.640Z"
    }
   },
   "outputs": [],
   "source": [
    "# length of dataframe\n",
    "len(tweets_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:56.644Z"
    }
   },
   "outputs": [],
   "source": [
    "# length of tweets only\n",
    "len(tweets_0[np.logical_and(tweets_0.reply == 0, tweets_0.retweet == 0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:56.649Z"
    }
   },
   "outputs": [],
   "source": [
    "# number of retweets\n",
    "len(tweets_0[tweets_0.retweet == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:56.654Z"
    }
   },
   "outputs": [],
   "source": [
    "# number of replies\n",
    "len(tweets_0[tweets_0.reply == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:56.660Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get only the first time a unique user tweet/retweet or replies with relevant #hash\n",
    "unique_users = tweets.drop_duplicates(subset=\"user_screen_name\", keep=\"first\")\n",
    "\n",
    "# sort by date\n",
    "unique_users = unique_users.sort_values(by=\"date\")\n",
    "\n",
    "# reset index\n",
    "unique_users = unique_users.reset_index()\n",
    "\n",
    "# get cumulative distribution, each user adds 1 to the count\n",
    "count_cumulative = [i + 1 for i in range(len(unique_users))]\n",
    "prop_cumulative = [i / len(unique_users) for i in range(len(unique_users))]\n",
    "\n",
    "# plot the distribution\n",
    "plt.plot(unique_users.date, prop_cumulative)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"date\")\n",
    "plt.ylabel(\"$N_a/N$\")\n",
    "plt.title(\"Fraction of recruited users over time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:56.665Z"
    }
   },
   "outputs": [],
   "source": [
    "# get days with most frequent tweets\n",
    "unique_users.date.dt.date.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:56.671Z"
    }
   },
   "outputs": [],
   "source": [
    "## DO THE SAME ABOVE FOR COMBINATIONS OF TWEETS, RETWEETS, and REPLIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:56.676Z"
    }
   },
   "outputs": [],
   "source": [
    "# number of unique users\n",
    "# Excluding retweets\n",
    "len(unique_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:56.681Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add date column to tweets data (without time)\n",
    "tweets['date'] = [tweets.date[i].date() for i in range(len(tweets))]\n",
    "\n",
    "# Number of tweets and unique tweets and replies\n",
    "# No retweets\n",
    "print(\"No. of tweets/replies:\", len(tweets))\n",
    "print(\"No. of unique tweets/replies:\", len(tweets.text.unique()))\n",
    "\n",
    "# Add date to nodes data for filtering say of degrees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:56.685Z"
    }
   },
   "outputs": [],
   "source": [
    "# filter only those in unified_followers dataset\n",
    "users_list = list(unified_followers.keys())\n",
    "tweets_2 = tweets[tweets.user_screen_name.isin(\n",
    "    users_list)].reset_index(drop=True)\n",
    "tweets_2 = tweets_2.drop(\"index\", axis=1)\n",
    "tweets_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:56.692Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check for similar tweet_id values - there shouldn't be duplicates\n",
    "# WE CAN REMOVE THIS\n",
    "for i in range(len(tweets)):\n",
    "    if len(tweets[tweets.tweet_id == tweets.tweet_id.values[i]]) > 1:\n",
    "        display(tweets[tweets.tweet_id == tweets.tweet_id.values[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:56.697Z"
    }
   },
   "outputs": [],
   "source": [
    "# No. of filtered tweets and users\n",
    "len(tweets_2), len(tweets_2.user_screen_name.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Only Users that Tweeted the #Hashtag and their Followers (who may or may not have tweeted the #Hashtag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asymmetric Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:57.047Z"
    }
   },
   "outputs": [],
   "source": [
    "# get list of unique user_screen_name in pandas dataframe\n",
    "# this is the list that is both in the followers list and in the tweets data\n",
    "# No yet concern for TIME so WE USE tweets_0\n",
    "# tweets, replies, and retweets\n",
    "final_users = list(tweets_0.user_screen_name.unique())\n",
    "len(final_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:57.052Z"
    }
   },
   "outputs": [],
   "source": [
    "# filter user - follower dictionary to include only those selected above\n",
    "# users_followers = {k: unified_followers[k] for k in final_users}\n",
    "# Get the intersection of user:followers list and the tweets\n",
    "\n",
    "# some users in the dataframe has not been scraped yet\n",
    "users_followers = {}\n",
    "for k in final_users:\n",
    "    try: users_followers[k] = unified_followers[k]\n",
    "    except: continue\n",
    "\n",
    "# these are the keys only\n",
    "len(users_followers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:57.057Z"
    }
   },
   "outputs": [],
   "source": [
    "# make a network using the filtered users_followers above\n",
    "# load Graph of followers\n",
    "# includes nodes that may not have tweeted about the protest\n",
    "G = nx.from_dict_of_lists(users_followers, create_using=nx.DiGraph())\n",
    "\n",
    "# Use spring layout\n",
    "# pos = nx.spring_layout(G)\n",
    "\n",
    "len(G.nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:57.455Z"
    }
   },
   "outputs": [],
   "source": [
    "# out degree\n",
    "degs = [k for n, k in G.out_degree]\n",
    "avg_deg = np.mean(degs)\n",
    "min_deg = np.min(degs)\n",
    "max_deg = np.max(degs)\n",
    "\n",
    "vals = {\"avg_degree\":[avg_deg], \"min_degree\":[min_deg], \"max_degree\":[max_deg]}\n",
    "pd.DataFrame.from_dict(vals).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:57.461Z"
    }
   },
   "outputs": [],
   "source": [
    "# in degree\n",
    "degs = [k for n, k in G.in_degree]\n",
    "avg_deg = np.mean(degs)\n",
    "min_deg = np.min(degs)\n",
    "max_deg = np.max(degs)\n",
    "\n",
    "vals = {\"avg_degree\":[avg_deg], \"min_degree\":[min_deg], \"max_degree\":[max_deg]}\n",
    "pd.DataFrame.from_dict(vals).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:57.467Z"
    }
   },
   "outputs": [],
   "source": [
    "# degree\n",
    "degs = [k for n, k in G.degree]\n",
    "avg_deg = np.mean(degs)\n",
    "min_deg = np.min(degs)\n",
    "max_deg = np.max(degs)\n",
    "\n",
    "vals = {\"avg_degree\":[avg_deg], \"min_degree\":[min_deg], \"max_degree\":[max_deg]}\n",
    "pd.DataFrame.from_dict(vals).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degree Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:57.653Z"
    }
   },
   "outputs": [],
   "source": [
    "# out degree distribution\n",
    "degs = [v for k, v in G.out_degree]\n",
    "\n",
    "# count\n",
    "deg_count = Counter(degs)\n",
    "\n",
    "# plot\n",
    "plt.loglog(deg_count.keys(), deg_count.values(), 'o', alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"out_degree\")\n",
    "plt.ylabel(\"num nodes\")\n",
    "plt.title(\"Out degree distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:57.659Z"
    }
   },
   "outputs": [],
   "source": [
    "# in degree distribution\n",
    "degs = [v for k, v in G.in_degree]\n",
    "\n",
    "# count\n",
    "deg_count = Counter(degs)\n",
    "\n",
    "# plot\n",
    "plt.loglog(deg_count.keys(), deg_count.values(), 'o', alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"in_degree\")\n",
    "plt.ylabel(\"num nodes\")\n",
    "plt.title(\"In degree distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:57.665Z"
    }
   },
   "outputs": [],
   "source": [
    "# degree distribution\n",
    "degs = [v for k, v in G.degree]\n",
    "\n",
    "# count\n",
    "deg_count = Counter(degs)\n",
    "\n",
    "# plot\n",
    "plt.loglog(deg_count.keys(), deg_count.values(), 'o', alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"degree\")\n",
    "plt.ylabel(\"num nodes\")\n",
    "plt.title(\"Degree distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:57.670Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Use spring layout\n",
    "# pos = nx.spring_layout(G)\n",
    "\n",
    "# # draw graph\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# nx.draw(G, pos=pos, node_size=2, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetric Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:57.937Z"
    }
   },
   "outputs": [],
   "source": [
    "# filter only symmetric\n",
    "sym_users_followers = filter_symmetric_from_dict(users_followers)\n",
    "len(sym_users_followers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:57.943Z"
    }
   },
   "outputs": [],
   "source": [
    "# load Graph of followers\n",
    "H = nx.from_dict_of_lists(sym_users_followers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degree Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:58.207Z"
    }
   },
   "outputs": [],
   "source": [
    "# degree distribution\n",
    "degs = [v for k, v in H.degree]\n",
    "\n",
    "# count\n",
    "deg_count = Counter(degs)\n",
    "\n",
    "# plot\n",
    "plt.plot(deg_count.keys(), deg_count.values(), 'o', alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"degree\")\n",
    "plt.ylabel(\"num nodes\")\n",
    "plt.title(\"Degree distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:58.214Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use spring layout\n",
    "pos = nx.spring_layout(H)\n",
    "\n",
    "# draw graph\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# draw graph\n",
    "nx.draw(H, pos=pos, node_size=2, with_labels=False,\n",
    "        font_color='b', font_size=20)\n",
    "\n",
    "# WE CAN'T SCRAPE ALL, LOTS OF MISSING LINKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:58.221Z"
    }
   },
   "outputs": [],
   "source": [
    "# top degrees in symmetric networks\n",
    "ind = np.argsort([v for k, v in H.degree])\n",
    "np.array([k for k, v in H.degree])[ind][::-1][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Only the Networks Whose Nodes Tweeted About the #Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:58.597Z"
    }
   },
   "outputs": [],
   "source": [
    "# get only nodes (following and followers) that are in a specific list\n",
    "def filter_users_from_dic(followers_dic, user_list):\n",
    "    # make dictionary of filtered keys\n",
    "    # filter user - follower dictionary to include only those that tweeted\n",
    "    filtered_dic = {}\n",
    "    for k in user_list:\n",
    "        try: filtered_dic[k] = followers_dic[k]\n",
    "        except: continue\n",
    "\n",
    "    # filter values in dictionary - select only those in user_list\n",
    "    for k, v in filtered_dic.items():\n",
    "        filtered_dic[k] = [i for i in filtered_dic[k] if i in user_list]\n",
    "\n",
    "    return filtered_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asymmetric Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:58.878Z"
    }
   },
   "outputs": [],
   "source": [
    "# get list of unique user_screen_name in pandas dataframe\n",
    "# this is the list that is both in the followers list and in the tweets data\n",
    "# No yet concern for TIME so WE USE tweets_0\n",
    "# tweets, replies, and retweets\n",
    "final_users = list(tweets_0.user_screen_name.unique())\n",
    "len(final_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:58.885Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filter only the nodes in final_users list (tweeter data)\n",
    "users_followers = filter_users_from_dic(unified_followers, final_users)\n",
    "len(users_followers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:58.893Z"
    }
   },
   "outputs": [],
   "source": [
    "# make a network using the filtered users_followers above\n",
    "# load Graph of followers\n",
    "# includes nodes that may not have tweeted about the protest\n",
    "G = nx.from_dict_of_lists(users_followers, create_using=nx.DiGraph())\n",
    "\n",
    "# Use spring layout\n",
    "# pos = nx.spring_layout(G)\n",
    "\n",
    "len(G.nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:59.184Z"
    }
   },
   "outputs": [],
   "source": [
    "# out degree\n",
    "degs = [k for n, k in G.out_degree]\n",
    "avg_deg = np.mean(degs)\n",
    "min_deg = np.min(degs)\n",
    "max_deg = np.max(degs)\n",
    "\n",
    "vals = {\"avg_degree\":[avg_deg], \"min_degree\":[min_deg], \"max_degree\":[max_deg]}\n",
    "pd.DataFrame.from_dict(vals).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:59.190Z"
    }
   },
   "outputs": [],
   "source": [
    "# in degree\n",
    "degs = [k for n, k in G.in_degree]\n",
    "avg_deg = np.mean(degs)\n",
    "min_deg = np.min(degs)\n",
    "max_deg = np.max(degs)\n",
    "\n",
    "vals = {\"avg_degree\":[avg_deg], \"min_degree\":[min_deg], \"max_degree\":[max_deg]}\n",
    "pd.DataFrame.from_dict(vals).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:59.196Z"
    }
   },
   "outputs": [],
   "source": [
    "# degree\n",
    "degs = [k for n, k in G.degree]\n",
    "avg_deg = np.mean(degs)\n",
    "min_deg = np.min(degs)\n",
    "max_deg = np.max(degs)\n",
    "\n",
    "vals = {\"avg_degree\":[avg_deg], \"min_degree\":[min_deg], \"max_degree\":[max_deg]}\n",
    "pd.DataFrame.from_dict(vals).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degree Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:59.498Z"
    }
   },
   "outputs": [],
   "source": [
    "# out degree distribution\n",
    "degs = [v for k, v in G.out_degree]\n",
    "\n",
    "# count\n",
    "deg_count = Counter(degs)\n",
    "\n",
    "# plot\n",
    "plt.loglog(deg_count.keys(), deg_count.values(), 'o', alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"out_degree\")\n",
    "plt.ylabel(\"num nodes\")\n",
    "plt.title(\"Out degree distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:59.509Z"
    }
   },
   "outputs": [],
   "source": [
    "# in degree distribution\n",
    "degs = [v for k, v in G.in_degree]\n",
    "\n",
    "# count\n",
    "deg_count = Counter(degs)\n",
    "\n",
    "# plot\n",
    "plt.loglog(deg_count.keys(), deg_count.values(), 'o', alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"in_degree\")\n",
    "plt.ylabel(\"num nodes\")\n",
    "plt.title(\"In degree distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:59.516Z"
    }
   },
   "outputs": [],
   "source": [
    "# degree distribution\n",
    "degs = [v for k, v in G.degree]\n",
    "\n",
    "# count\n",
    "deg_count = Counter(degs)\n",
    "\n",
    "# plot\n",
    "plt.loglog(deg_count.keys(), deg_count.values(), 'o', alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"degree\")\n",
    "plt.ylabel(\"num nodes\")\n",
    "plt.title(\"Degree distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:59.525Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Use spring layout\n",
    "# pos = nx.spring_layout(G)\n",
    "\n",
    "# # draw graph\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# nx.draw(G, pos=pos, node_size=2, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetric Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:59.918Z"
    }
   },
   "outputs": [],
   "source": [
    "# filter only symmetric\n",
    "sym_users_followers = filter_symmetric_from_dict(users_followers)\n",
    "len(sym_users_followers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:13:59.931Z"
    }
   },
   "outputs": [],
   "source": [
    "# load Graph of followers\n",
    "H = nx.from_dict_of_lists(sym_users_followers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degree Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:00.234Z"
    }
   },
   "outputs": [],
   "source": [
    "# degree distribution\n",
    "degs = [v for k, v in H.degree]\n",
    "\n",
    "# count\n",
    "deg_count = Counter(degs)\n",
    "\n",
    "x = np.array(list(deg_count.keys()))\n",
    "inds = np.argsort(x)\n",
    "x = x[inds]\n",
    "y = np.array(list(deg_count.values()))\n",
    "y = y[inds]\n",
    "\n",
    "# plot\n",
    "plt.plot(x, y, '-', alpha=1)\n",
    "\n",
    "plt.xlabel(\"degree\")\n",
    "plt.ylabel(\"num nodes\")\n",
    "plt.title(\"Degree distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:00.239Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use spring layout\n",
    "pos = nx.spring_layout(H)\n",
    "\n",
    "# draw graph\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# draw graph\n",
    "nx.draw(H, pos=pos, node_size=2, with_labels=False,\n",
    "        font_color='b', font_size=20)\n",
    "\n",
    "# WE CAN'T SCRAPE ALL, LOTS OF MISSING LINKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:00.243Z"
    }
   },
   "outputs": [],
   "source": [
    "# top degrees in symmetric networks\n",
    "ind = np.argsort([v for k, v in H.degree])\n",
    "np.array([k for k, v in H.degree])[ind][::-1][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T19:56:51.715032Z",
     "start_time": "2019-02-25T19:56:49.556444Z"
    }
   },
   "source": [
    "## Recruitment Thresholds Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Threshold Distribution - Asymmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:00.658Z"
    }
   },
   "outputs": [],
   "source": [
    "## WE DON'T HAVE DATE FOR RETWEETS. HOW CAN WE SOLVE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:00.665Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_thresholds(following_counts, unique_users, following_dict):\n",
    "    # input is a dictionary of {users : num of following}\n",
    "    # input is unique users to select\n",
    "\n",
    "    # initiate threshold dict\n",
    "    thresh_dic = {}\n",
    "\n",
    "    # Count the number of friends who activated at the time of activation, ka\n",
    "    for user in list(following_counts.keys()):\n",
    "\n",
    "        # check if user in user_screen_name in the dataset otherwise continue\n",
    "        if user not in unique_users.user_screen_name.values:\n",
    "            continue\n",
    "\n",
    "        date_activated = unique_users[unique_users.user_screen_name == user].date\n",
    "\n",
    "        # filter users whose date < date activated\n",
    "        preactivated_users = unique_users[unique_users.date.values <\n",
    "                                          date_activated.values]\n",
    "\n",
    "        # filter only those being followed by user\n",
    "        # DOUBLE CHECK THIS THIS SHOULDNT BE UNIFIED FOLLOWING\n",
    "        _ = preactivated_users[preactivated_users.user_screen_name.isin(\n",
    "            following_dict[user])]\n",
    "\n",
    "        # get number of preactivated users that are being followed\n",
    "        ka = len(_)\n",
    "\n",
    "        # get number of people followed\n",
    "        k = following_counts[user]\n",
    "\n",
    "        if ka > k:\n",
    "            print(ka, k)\n",
    "            print(user)\n",
    "\n",
    "        # if no connections, remove\n",
    "        if k == 0:\n",
    "            continue\n",
    "\n",
    "        # get threshold\n",
    "        thresh = ka / k\n",
    "\n",
    "        # append to dictionary\n",
    "        thresh_dic[user] = thresh\n",
    "\n",
    "    return thresh_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:00.671Z"
    }
   },
   "outputs": [],
   "source": [
    "# get following counts\n",
    "following_counts = {k: len(v) for k, v in unified_following.items()}\n",
    "\n",
    "# unique_users are unfiltered. Symmetric\n",
    "unique_users = tweets.drop_duplicates(subset=\"user_screen_name\", keep=\"first\")\n",
    "\n",
    "# show some threshold values\n",
    "thresholds = get_thresholds(following_counts, unique_users, unified_following)\n",
    "\n",
    "# display\n",
    "len(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:00.681Z"
    }
   },
   "outputs": [],
   "source": [
    "# mean threshold\n",
    "np.mean([v for k, v in thresholds.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:00.687Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot distribution proportion of nodes vs ka/k\n",
    "thresh = [v for k, v in thresholds.items()]\n",
    "thresh_counts = Counter(thresh)\n",
    "x = np.array(list(thresh_counts.keys()))\n",
    "inds = np.argsort(x)\n",
    "x_asym = x[inds]\n",
    "y_asym = np.array(list(thresh_counts.values()))[inds]\n",
    "plt.semilogy(x_asym, y_asym)\n",
    "plt.xlabel(\"ka/k\")\n",
    "plt.ylabel(\"num nodes\")\n",
    "plt.title(\"Num nodes vs ka/k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Threshold Distribution - Symmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:00.892Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filter the network for symmetric only\n",
    "# list of users and who they follow in symmetric network\n",
    "symm_following = filter_symmetric_from_dict(unified_following)\n",
    "\n",
    "# make graph\n",
    "H = nx.from_dict_of_lists(symm_following, create_using=nx.DiGraph())\n",
    "\n",
    "len(H.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:00.898Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get following counts\n",
    "following_counts = {k: len(v) for k, v in symm_following.items()}\n",
    "\n",
    "# unique_users are unfiltered. Symmetric\n",
    "unique_users = unique_users\n",
    "\n",
    "# show some threshold values\n",
    "thresholds = get_thresholds(following_counts, unique_users, symm_following)\n",
    "\n",
    "len(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:00.903Z"
    }
   },
   "outputs": [],
   "source": [
    "# mean threshold\n",
    "np.mean([v for k, v in thresholds.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:00.909Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot distribution proportion of nodes vs ka/k\n",
    "thresh = [v for k, v in thresholds.items()]\n",
    "thresh_counts = Counter(thresh)\n",
    "x = np.array(list(thresh_counts.keys()))\n",
    "inds = np.argsort(x)\n",
    "x_sym = x[inds]\n",
    "y_sym = np.array(list(thresh_counts.values()))[inds]\n",
    "plt.semilogy(x_sym, y_sym)\n",
    "plt.xlabel(\"ka/k\")\n",
    "plt.ylabel(\"num nodes\")\n",
    "plt.title(\"Num nodes vs ka/k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:00.914Z"
    }
   },
   "outputs": [],
   "source": [
    "# CHANGE THIS TO PROPORTIONS\n",
    "\n",
    "plt.semilogy(x_sym, y_sym, \"-\", alpha=0., ms=4)\n",
    "plt.semilogy(x_asym, y_asym, \"-\", alpha=0., ms=4)\n",
    "\n",
    "# f, ax = plt.subplots()\n",
    "# ax.set(xscale=\"log\", yscale=\"log\")\n",
    "\n",
    "plt.bar(x_asym, y_asym, label=\"asymmetric\", width=0.02, alpha=0.5)\n",
    "plt.bar(x_sym, y_sym, label=\"symmetric\", width=0.02, alpha=0.5)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"ka/k\")\n",
    "plt.ylabel(\"num nodes\")\n",
    "plt.title(\"Num nodes vs ka/k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that retaining the symmetric network is like removing ifluencers and media (with huge number of followers but few users followed), or retaining groups of friends in the network, the threshold distribution shifts to higher threshold values. It might be that people in the symmetric subgraph needed more pressure to join the protest / be recruited. Media and influencers could have played a role in activating a significant portion of the users (lower required ka/k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Low, Med, and High Threshold Users Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:01.506Z"
    }
   },
   "outputs": [],
   "source": [
    "# TO DO THIS!!!!\n",
    "# ASYMMETRIC\n",
    "# get following counts\n",
    "following_counts = {k: len(v) for k, v in unified_following.items()}\n",
    "\n",
    "# unique_users are unfiltered. Symmetric\n",
    "unique_users = tweets.drop_duplicates(subset=\"user_screen_name\", keep=\"first\")\n",
    "\n",
    "# show some threshold values\n",
    "thresholds = get_thresholds(following_counts, unique_users, unified_following)\n",
    "\n",
    "# display\n",
    "len(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:01.512Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the date\n",
    "unique_users_date = unique_users.loc[:, [\"user_screen_name\", \"date\"]]\n",
    "unique_users_date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:01.518Z"
    }
   },
   "outputs": [],
   "source": [
    "# get a dataframe of threshold values\n",
    "df_thresh = pd.DataFrame.from_dict(thresholds, orient=\"index\").reset_index()\n",
    "df_thresh.columns = [\"user_screen_name\", \"threshold\"]\n",
    "df_thresh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:01.523Z"
    }
   },
   "outputs": [],
   "source": [
    "# combine user_screen_name, date, and thresholds\n",
    "df_thresh_date = pd.merge(unique_users_date, df_thresh,\n",
    "         left_on=\"user_screen_name\", right_on=\"user_screen_name\")\n",
    "df_thresh_date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:01.528Z"
    }
   },
   "outputs": [],
   "source": [
    "# max thresholds\n",
    "low = 0.2\n",
    "med = 0.5\n",
    "\n",
    "# filter thresholds of users (low thresholds)\n",
    "low_thresh = df_thresh_date[df_thresh_date.threshold <= low].reset_index(\n",
    "    drop=True)\n",
    "\n",
    "# get counts per date only (distribution)\n",
    "low_thresh = low_thresh.groupby(\"date\").count().reset_index()\n",
    "\n",
    "# filter thresholds of users (med thresholds)\n",
    "med_thresh = df_thresh_date[np.logical_and(\n",
    "    low < df_thresh_date.threshold, df_thresh_date.threshold <= med)].reset_index(drop=True)\n",
    "\n",
    "# get counts per date only (distribution)\n",
    "med_thresh = med_thresh.groupby(\"date\").count().reset_index()\n",
    "\n",
    "# filter thresholds of users (high thresholds)\n",
    "high_thresh = df_thresh_date[med < df_thresh_date.threshold].reset_index(\n",
    "    drop=True)\n",
    "\n",
    "# get counts per date only (distribution)\n",
    "high_thresh = high_thresh.groupby(\"date\").count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:01.533Z"
    }
   },
   "outputs": [],
   "source": [
    "# size of each threshold category\n",
    "np.sum(low_thresh.threshold), np.sum(med_thresh.threshold), np.sum(high_thresh.threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:01.537Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot proportions (RIGHT NOW THRESHOLDS ARE PLOTTED)\n",
    "\n",
    "x = low_thresh.date\n",
    "y = low_thresh.threshold / np.sum(low_thresh.threshold)\n",
    "plt.plot(x, y, label=f\"low $\\leq$ {low}\")\n",
    "\n",
    "x = med_thresh.date\n",
    "y = med_thresh.threshold / np.sum(med_thresh.threshold)\n",
    "plt.plot(x, y, label=f\"{low } < med $\\leq$ {med}\")\n",
    "\n",
    "x = high_thresh.date\n",
    "y = high_thresh.threshold / np.sum(high_thresh.threshold)\n",
    "plt.plot(x, y, label=f\"{med} < high\")\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(\"date\")\n",
    "plt.ylabel(\"proportion of users who activated\")\n",
    "plt.title(\"Proportion of users who activated over time for three threshold categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:01.542Z"
    }
   },
   "outputs": [],
   "source": [
    "# SYMMETRIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Distribution Before and After Protest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:01.771Z"
    }
   },
   "outputs": [],
   "source": [
    "# threshold before protest\n",
    "tweets_before = tweets_0[tweets_0.date < datetime(2013, 8, 26)].reset_index(drop=True)\n",
    "len(tweets_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:01.776Z"
    }
   },
   "outputs": [],
   "source": [
    "# get following counts\n",
    "# asymmetric\n",
    "following_counts = {k: len(v) for k, v in unified_following.items()}\n",
    "\n",
    "# unique_users are unfiltered. Symmetric\n",
    "unique_users = tweets_before.drop_duplicates(subset=\"user_screen_name\", keep=\"first\")\n",
    "\n",
    "# show some threshold values\n",
    "thresholds = get_thresholds(following_counts, unique_users, unified_following)\n",
    "\n",
    "# display\n",
    "len(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:01.781Z"
    }
   },
   "outputs": [],
   "source": [
    "# mean threshold\n",
    "np.mean([v for k, v in thresholds.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:01.787Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot distribution proportion of nodes vs ka/k\n",
    "# proportion of nodes that activated before protest\n",
    "thresh = [v for k, v in thresholds.items()]\n",
    "thresh_counts = Counter(thresh)\n",
    "x = np.array(list(thresh_counts.keys()))\n",
    "inds = np.argsort(x)\n",
    "x_asym_before = x[inds]\n",
    "y_asym_before = np.array(list(thresh_counts.values()))[inds]\n",
    "y_asym_before = y_asym_before/np.sum(y_asym_before)\n",
    "plt.semilogy(x_asym_before, y_asym_before)\n",
    "plt.xlabel(\"ka/k\")\n",
    "plt.ylabel(\"proportion of nodes that activated during that time frame\")\n",
    "plt.title(\"Prop nodes vs ka/k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:01.792Z"
    }
   },
   "outputs": [],
   "source": [
    "# threshold after protest\n",
    "tweets_after = tweets_0[tweets_0.date >= datetime(2013, 8, 26)].reset_index(drop=True)\n",
    "len(tweets_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:01.797Z"
    }
   },
   "outputs": [],
   "source": [
    "# get following counts\n",
    "# asymmetric\n",
    "following_counts = {k: len(v) for k, v in unified_following.items()}\n",
    "\n",
    "# unique_users are unfiltered. Symmetric\n",
    "unique_users = tweets_after.drop_duplicates(subset=\"user_screen_name\", keep=\"first\")\n",
    "\n",
    "# show some threshold values\n",
    "thresholds = get_thresholds(following_counts, unique_users, unified_following)\n",
    "\n",
    "# display\n",
    "len(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:01.803Z"
    }
   },
   "outputs": [],
   "source": [
    "# mean threshold\n",
    "np.mean([v for k, v in thresholds.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:01.808Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot distribution proportion of nodes vs ka/k\n",
    "thresh = [v for k, v in thresholds.items()]\n",
    "thresh_counts = Counter(thresh)\n",
    "x = np.array(list(thresh_counts.keys()))\n",
    "inds = np.argsort(x)\n",
    "x_asym_after = x[inds]\n",
    "y_asym_after = np.array(list(thresh_counts.values()))[inds]\n",
    "y_asym_after = y_asym_after/np.sum(y_asym_after)\n",
    "plt.semilogy(x_asym_after, y_asym_after)\n",
    "plt.xlabel(\"ka/k\")\n",
    "plt.ylabel(\"num nodes\")\n",
    "plt.title(\"Num nodes vs ka/k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:01.814Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.semilogy(x_asym_before, y_asym_before, 'o-', alpha=0.7, label=\"before protest\")\n",
    "plt.semilogy(x_asym_after, y_asym_after,'o-', alpha=0.7, label=\"after protest\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.semilogy(x_asym_before, y_asym_before, 'x', alpha=0.7, label=\"before protest\")\n",
    "plt.semilogy(x_asym_after, y_asym_after,'o', alpha=0.7, label=\"after protest\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_asym_before, y_asym_before, 'x', alpha=0.7, label=\"before protest\")\n",
    "plt.plot(x_asym_after, y_asym_after,'o', alpha=0.7, label=\"after protest\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.semilogy(x_asym_after, y_asym_after, alpha=0.)\n",
    "plt.bar(x_asym_before, y_asym_before, alpha=0.3, width=0.03, label=\"before protest\")\n",
    "plt.bar(x_asym_after, y_asym_after, alpha=0.3, width=0.03, label=\"after protest\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "sns.distplot(x_asym_before, label=\"before protest\", kde=False, norm_hist=True, hist=True,)\n",
    "sns.distplot(x_asym_after, label=\"after protest\", kde=False, norm_hist=True, hist=True,)\n",
    "plt.xlabel(\"ka/k\")\n",
    "plt.ylabel(\"proportion of nodes that activated during that time frame\")\n",
    "plt.title(\"Distribution of ka/k before and after protest\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recruitment Bursts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Cascade Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:02.584Z"
    }
   },
   "outputs": [],
   "source": [
    "# How to identify cascade? retweet of same message?\n",
    "    # Count how many retweeted same message = cascade size?\n",
    "    # Count how many used the same #hashtag - cascade size?\n",
    "    # Add k core attributes per node\n",
    "    # Find K core of who started = seed/leader\n",
    "    # Associate cascade size with k core of seed/leader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:02.592Z"
    }
   },
   "outputs": [],
   "source": [
    "# count retweets for each parent id\n",
    "cascade_sizes = tweets_0.groupby(\n",
    "    by=\"parent_tweet_id\").count().reset_index().iloc[:, :2].date.values\n",
    "\n",
    "# Should we add this?\n",
    "# # we add the original tweet\n",
    "# cascade_sizes = cascade_sizes + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:02.601Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array(list(Counter(cascade_sizes).keys()))\n",
    "inds = np.argsort(x)\n",
    "x = x[inds]\n",
    "y = np.array(list(Counter(cascade_sizes).values()))[inds]\n",
    "plt.loglog(x, y, marker=\".\", )\n",
    "plt.xlabel(\"cascade size (number of retweets + original tweet)\")\n",
    "plt.ylabel(\"number of nodes\")\n",
    "plt.title(\"Distribution of cascade size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:02.606Z"
    }
   },
   "outputs": [],
   "source": [
    "# WE CAN CHECK WHO ARE THE NODES THAT REACHED MANY PEOPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Centrality (K Core) vs Cascade Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:02.923Z"
    }
   },
   "outputs": [],
   "source": [
    "# make a dictionary of parent tweet_id and cascade size\n",
    "# Check in the asymmetric network and symmetric network\n",
    "# will associate parent_tweet_id with original tweeter and calculate k core\n",
    "\n",
    "cascade_df = tweets_0.groupby(\n",
    "    by=\"parent_tweet_id\").count().reset_index().iloc[:, :2]\n",
    "# rename column to cascade\n",
    "cascade_df = cascade_df.rename(columns={\"date\": \"cascade\"})\n",
    "cascade_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:02.928Z"
    }
   },
   "outputs": [],
   "source": [
    "# k cores in asymmetric network\n",
    "# from G above - includes followers who do not follow back and did not tweet\n",
    "# about the protest\n",
    "len(G.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:02.933Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove self-loops\n",
    "G.remove_edges_from(nx.selfloop_edges(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:02.938Z"
    }
   },
   "outputs": [],
   "source": [
    "len(G.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:02.943Z"
    }
   },
   "outputs": [],
   "source": [
    "# k cores in asymmetric network - includes those who did not tweet about the protest\n",
    "# {name: k_core, name: k_core}\n",
    "k_cores_dict = nx.core_number(G)\n",
    "\n",
    "# SAVE KCORE AS ATTRIBUTE OF NODE\n",
    "len(k_cores_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:02.949Z"
    }
   },
   "outputs": [],
   "source": [
    "# k_core per users\n",
    "k_core_df = pd.DataFrame.from_dict(\n",
    "    k_cores_dict, orient='index').reset_index()\n",
    "len(k_core_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:02.954Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the parent user screen name\n",
    "cascade_df_2 = pd.merge(unique_users, cascade_df, left_on=\"tweet_id\",\n",
    "                        right_on=\"parent_tweet_id\").loc[:, [\"user_screen_name\", \"cascade\"]]\n",
    "cascade_df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:02.959Z"
    }
   },
   "outputs": [],
   "source": [
    "k_core_users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:02.965Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge cascade size and kcore\n",
    "df_cascade_kcore = pd.merge(k_core_users, cascade_df_2, left_on=\"index\",\n",
    "         right_on=\"user_screen_name\")[[0, \"cascade\"]]\n",
    "df_cascade_kcore.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:03.004Z"
    }
   },
   "outputs": [],
   "source": [
    "len(df_cascade_kcore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:03.019Z"
    }
   },
   "outputs": [],
   "source": [
    "x = df_cascade_kcore[0]\n",
    "inds = np.argsort(x)\n",
    "x = x[inds]\n",
    "y = df_cascade_kcore[\"cascade\"][inds]\n",
    "\n",
    "plt.plot(x, y/len(y), 'o')\n",
    "plt.title(\"cascade vs k-core\")\n",
    "plt.xlabel(\"k core\")\n",
    "plt.ylabel(\"$N_c$/$N$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:03.032Z"
    }
   },
   "outputs": [],
   "source": [
    "# average cascade size vs k core\n",
    "cascade_kcore_mean = df_cascade_kcore.groupby(by=0).mean().reset_index()\n",
    "\n",
    "x = cascade_kcore_mean[0]\n",
    "inds = np.argsort(x)\n",
    "x = x[inds]\n",
    "y = cascade_kcore_mean[\"cascade\"][inds]\n",
    "\n",
    "plt.plot(x, y, '-')\n",
    "plt.title(\"cascade vs k-core\")\n",
    "plt.xlabel(\"k core\")\n",
    "plt.ylabel(\"Average cascade size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average cascade size correlates positively with k core size. This may mean that tweets from users in higher k cores reach more people than users in lower k cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:03.351Z"
    }
   },
   "outputs": [],
   "source": [
    "# correlate with kcore with degrees\n",
    "# degree per users\n",
    "degrees = dict(G.degree)\n",
    "\n",
    "degree_df = pd.DataFrame.from_dict(\n",
    "    degrees, orient='index').reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:03.357Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge cascade size and kcore\n",
    "df_degree_k_core = pd.merge(degree_df, cascade_df_2, left_on=\"index\",\n",
    "         right_on=\"user_screen_name\")[[0, \"cascade\"]]\n",
    "df_degree_k_core.columns = [\"degree\", \"cascade\"]\n",
    "df_degree_k_core.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:03.363Z"
    }
   },
   "outputs": [],
   "source": [
    "len(df_degree_k_core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:03.369Z"
    }
   },
   "outputs": [],
   "source": [
    "# degree vs k core\n",
    "\n",
    "x = df_degree_k_core[\"degree\"]\n",
    "inds = np.argsort(x)\n",
    "x = x[inds]\n",
    "y = df_degree_k_core[\"cascade\"][inds]\n",
    "\n",
    "plt.loglog(x, y, 'o')\n",
    "plt.title(\"cascade vs degree\")\n",
    "plt.xlabel(\"degree\")\n",
    "plt.ylabel(\"cascade size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T11:14:03.375Z"
    }
   },
   "outputs": [],
   "source": [
    "# mean degree vs k core\n",
    "df_degree_k_core_mean = df_degree_k_core.groupby(by=\"degree\").mean().reset_index()\n",
    "\n",
    "x = df_degree_k_core_mean[\"degree\"]\n",
    "inds = np.argsort(x)\n",
    "x = x[inds]\n",
    "y = df_degree_k_core_mean[\"cascade\"][inds]\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.set(xscale=\"log\", yscale=\"log\")\n",
    "\n",
    "# sns.lmplot(\"degree\", \"cascade\", data=df_degree_k_core_mean)\n",
    "plt.loglog(x, y, 'o')\n",
    "plt.title(\"average cascade vs degree\")\n",
    "plt.xlabel(\"degree\")\n",
    "plt.ylabel(\"Average cascade size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
