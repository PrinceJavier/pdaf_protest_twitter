{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T03:35:28.215396Z",
     "start_time": "2019-03-09T03:35:20.863844Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run this for followers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    " \n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import json\n",
    "\n",
    "import requests\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://allofyourbases.com/2018/01/16/mining-twitter-with-selenium/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T03:35:30.882841Z",
     "start_time": "2019-03-09T03:35:28.217841Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run these function definitions\n",
    "def scrape_followers(driver):\n",
    "    # initial wait for the search results to load\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    try:\n",
    "        # wait until the first search result is found. Search results will be tweets, which are html list items and have the class='data-item-id':\n",
    "        wait.until(EC.visibility_of_element_located(\n",
    "            (By.CSS_SELECTOR, \"div[data-user-id]\")))\n",
    "\n",
    "        # scroll down to the last tweet until there are no more tweets:\n",
    "        while True:\n",
    "\n",
    "            # extract all the tweets:\n",
    "            followers = driver.find_elements_by_css_selector(\n",
    "                \"div[data-user-id]\")\n",
    "\n",
    "            # find number of visible tweets:\n",
    "            number_of_followers = len(followers)\n",
    "\n",
    "            # keep scrolling:\n",
    "            driver.execute_script(\n",
    "                \"arguments[0].scrollIntoView();\", followers[-1])\n",
    "\n",
    "            try:\n",
    "                # wait for more tweets to be visible:\n",
    "                wait.until(wait_for_more_than_n_elements_to_be_present(\n",
    "                    (By.CSS_SELECTOR, \"div[data-user-id]\"), number_of_followers))\n",
    "\n",
    "            except TimeoutException:\n",
    "                # if no more are visible the \"wait.until\" call will timeout. Catch the exception and exit the while loop:\n",
    "                break\n",
    "\n",
    "        # extract the html for the whole lot:\n",
    "        page_source = driver.page_source\n",
    "\n",
    "    except TimeoutException:\n",
    "\n",
    "        # if there are no search results then the \"wait.until\" call in the first \"try\" statement will never happen and it will time out. So we catch that exception and return no html.\n",
    "        page_source = None\n",
    "\n",
    "    return page_source\n",
    "\n",
    "\n",
    "def extract_followers(page_source):\n",
    "\n",
    "    soup = bs(page_source, 'lxml')\n",
    "\n",
    "    followers = []\n",
    "\n",
    "    for div in soup.find_all(\"a\", class_='fullname'):\n",
    "        follower = {\n",
    "            'screen_name': div['href'][1:],\n",
    "            'full_name': div.get_text().strip()\n",
    "        }\n",
    "\n",
    "        followers.append(follower)\n",
    "\n",
    "    return followers[1:]\n",
    "\n",
    "\n",
    "def init_driver():\n",
    "\n",
    "    # do not load images\n",
    "    chromeOptions = webdriver.ChromeOptions()\n",
    "    prefs = {'profile.managed_default_content_settings.images':2}\n",
    "    chromeOptions.add_experimental_option(\"prefs\", prefs)\n",
    "    \n",
    "    # initiate the driver:\n",
    "    driver = webdriver.Chrome(\n",
    "        '/Users/fernandojavier/Desktop/MSDS/Portfolio/Protest Spread in Twitter/chromedriver', \n",
    "        chrome_options=chromeOptions)\n",
    "    # set a default wait time for the browser [5 seconds here]:\n",
    "    driver.wait = WebDriverWait(driver, 5)\n",
    "\n",
    "    return driver\n",
    "\n",
    "\n",
    "def close_driver(driver):\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def login_twitter(driver, username, password):\n",
    "\n",
    "    # open the web page in the browser:\n",
    "    driver.get(\"https://twitter.com/login\")\n",
    "\n",
    "    # find the boxes for username and password\n",
    "    username_field = driver.find_element_by_class_name(\"js-username-field\")\n",
    "    password_field = driver.find_element_by_class_name(\"js-password-field\")\n",
    "\n",
    "    # enter your username:\n",
    "    username_field.send_keys(username)\n",
    "    driver.implicitly_wait(1)\n",
    "\n",
    "    # enter your password:\n",
    "    password_field.send_keys(password)\n",
    "    driver.implicitly_wait(1)\n",
    "\n",
    "    # click the \"Log In\" button:\n",
    "    driver.find_element_by_class_name(\"EdgeButtom--medium\").click()\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "class wait_for_more_than_n_elements_to_be_present(object):\n",
    "    def __init__(self, locator, count):\n",
    "        self.locator = locator\n",
    "        self.count = count\n",
    "\n",
    "    def __call__(self, driver):\n",
    "        try:\n",
    "            elements = EC._find_elements(driver, self.locator)\n",
    "            return len(elements) > self.count\n",
    "        except StaleElementReferenceException:\n",
    "            return False\n",
    "\n",
    "\n",
    "def search_twitter(driver, query):\n",
    "\n",
    "    # wait until the search box has loaded:\n",
    "    box = driver.wait.until(EC.presence_of_element_located((By.NAME, \"q\")))\n",
    "\n",
    "    # find the search box in the html:\n",
    "    driver.find_element_by_name(\"q\").clear()\n",
    "\n",
    "    # enter your search string in the search box:\n",
    "    box.send_keys(query)\n",
    "\n",
    "    # submit the query (like hitting return):\n",
    "    box.submit()\n",
    "\n",
    "    # initial wait for the search results to load\n",
    "    wait = WebDriverWait(driver, 5)\n",
    "\n",
    "    try:\n",
    "        # wait until the first search result is found. Search results will be tweets, which are html list items and have the class='data-item-id':\n",
    "        wait.until(EC.visibility_of_element_located(\n",
    "            (By.CSS_SELECTOR, \"li[data-item-id]\")))\n",
    "\n",
    "        # scroll down to the last tweet until there are no more tweets:\n",
    "        while True:\n",
    "\n",
    "            # extract all the tweets:\n",
    "            tweets = driver.find_elements_by_css_selector(\"li[data-item-id]\")\n",
    "\n",
    "            # find number of visible tweets:\n",
    "            number_of_tweets = len(tweets)\n",
    "\n",
    "            # keep scrolling:\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", tweets[-1])\n",
    "\n",
    "            try:\n",
    "                # wait for more tweets to be visible:\n",
    "                wait.until(wait_for_more_than_n_elements_to_be_present(\n",
    "                    (By.CSS_SELECTOR, \"li[data-item-id]\"), number_of_tweets))\n",
    "\n",
    "            except TimeoutException:\n",
    "                # if no more are visible the \"wait.until\" call will timeout. Catch the exception and exit the while loop:\n",
    "                break\n",
    "\n",
    "        # extract the html for the whole lot:\n",
    "        page_source = driver.page_source\n",
    "\n",
    "    except TimeoutException:\n",
    "\n",
    "        # if there are no search results then the \"wait.until\" call in the first \"try\" statement will never happen and it will time out. So we catch that exception and return no html.\n",
    "        page_source = None\n",
    "\n",
    "    return page_source\n",
    "\n",
    "\n",
    "def extract_tweets(page_source, driver):\n",
    "\n",
    "    soup = bs(page_source, 'lxml')\n",
    "\n",
    "    tweets = []\n",
    "    for li in soup.find_all(\"li\", class_='js-stream-item'):\n",
    "\n",
    "        # If our li doesn't have a tweet-id, we skip it as it's not going to be a tweet.\n",
    "        if 'data-item-id' not in li.attrs:\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            tweet = {\n",
    "                'tweet_id': li['data-item-id'],\n",
    "                'parent_tweet_id': None,\n",
    "                'text': None,\n",
    "                'user_id': None,\n",
    "                'user_screen_name': None,\n",
    "                'user_name': None,\n",
    "                'retweets': 0,\n",
    "                'likes': 0,\n",
    "                'replies': 0,\n",
    "                'timestamp': 0,\n",
    "                'date': '',\n",
    "                'reply': 0,\n",
    "                'retweet': 0\n",
    "            }\n",
    "\n",
    "            # Tweet Text\n",
    "            text_p = li.find(\"p\", class_=\"tweet-text\")\n",
    "            if text_p is not None:\n",
    "                tweet['text'] = text_p.get_text()\n",
    "\n",
    "            # Tweet User ID, User Screen Name, User Name\n",
    "            user_details_div = li.find(\"div\", class_=\"tweet\")\n",
    "            if user_details_div is not None:\n",
    "                tweet['user_id'] = user_details_div['data-user-id']\n",
    "                tweet['user_screen_name'] = user_details_div['data-screen-name']\n",
    "                tweet['user_name'] = user_details_div['data-name']\n",
    "\n",
    "            # Tweet Retweets\n",
    "            retweet_span = li.select(\n",
    "                \"span.ProfileTweet-action--retweet > span.ProfileTweet-actionCount\")\n",
    "            if retweet_span is not None and len(retweet_span) > 0:\n",
    "                tweet['retweets'] = int(\n",
    "                    retweet_span[0]['data-tweet-stat-count'])\n",
    "\n",
    "            # Tweet Likes\n",
    "            like_span = li.select(\n",
    "                \"span.ProfileTweet-action--favorite > span.ProfileTweet-actionCount\")\n",
    "            if like_span is not None and len(like_span) > 0:\n",
    "                tweet['likes'] = int(like_span[0]['data-tweet-stat-count'])\n",
    "\n",
    "            # Tweet Replies\n",
    "            reply_span = li.select(\n",
    "                \"span.ProfileTweet-action--reply > span.ProfileTweet-actionCount\")\n",
    "            if reply_span is not None and len(reply_span) > 0:\n",
    "                tweet['replies'] = int(reply_span[0]['data-tweet-stat-count'])\n",
    "\n",
    "            date_span = li.find(\"span\", class_=\"js-short-timestamp\")\n",
    "            if date_span is not None:\n",
    "                tweet['timestamp'] = date_span['data-time']\n",
    "                tweet['date'] = datetime.utcfromtimestamp(\n",
    "                    int(tweet['timestamp'])\n",
    "                ).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            tweets.append(tweet)\n",
    "\n",
    "            driver.execute_script(\n",
    "                \"document.getElementById('\" +\n",
    "                str(li['id']) +\n",
    "                \"').querySelector('.tweet').click()\")\n",
    "\n",
    "            wait = WebDriverWait(driver, 2)\n",
    "\n",
    "            try:\n",
    "                # wait until the first search result is found. Search results will be tweets, which are html list items and have the class='data-item-id':\n",
    "                wait.until(EC.visibility_of_element_located(\n",
    "                    (By.CSS_SELECTOR, \"div[data-has-parent-tweet]\")))\n",
    "\n",
    "                # scroll down to the last tweet until there are no more tweets:\n",
    "                while True:\n",
    "\n",
    "                    # extract all the tweets:\n",
    "                    replies = driver.find_elements_by_css_selector(\n",
    "                        \"div[data-has-parent-tweet]\")\n",
    "\n",
    "                    # find number of visible tweets:\n",
    "                    number_of_replies = len(replies)\n",
    "\n",
    "                    if number_of_replies == 0:\n",
    "                        break\n",
    "\n",
    "                    # keep scrolling:\n",
    "                    try:\n",
    "\n",
    "                        driver.execute_script(\n",
    "                            \"arguments[0].scrollIntoView();\", replies[-1])\n",
    "\n",
    "                    except Exception:\n",
    "                        break\n",
    "\n",
    "                    try:\n",
    "                        # wait for more tweets to be visible:\n",
    "                        wait.until(wait_for_more_than_n_elements_to_be_present(\n",
    "                            (By.CSS_SELECTOR, \"div[data-has-parent-tweet]\"), number_of_replies))\n",
    "\n",
    "                    except TimeoutException:\n",
    "                        # if no more are visible the \"wait.until\" call will timeout. Catch the exception and exit the while loop:\n",
    "                        break\n",
    "\n",
    "                # extract the html for the whole lot:\n",
    "                page_source_replies = driver.page_source\n",
    "\n",
    "            except TimeoutException:\n",
    "\n",
    "                # if there are no search results then the \"wait.until\" call in the first \"try\" statement will never happen and it will time out. So we catch that exception and return no html.\n",
    "                page_source_replies = None\n",
    "\n",
    "            soup_replies = bs(page_source_replies, 'lxml')\n",
    "\n",
    "            for div in soup_replies.find_all(\"div\", class_='permalink-descendant-tweet'):\n",
    "\n",
    "                # If our li doesn't have a tweet-id, we skip it as it's not going to be a tweet.\n",
    "                if 'data-item-id' not in div.attrs:\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    reply = {\n",
    "                        'tweet_id': div['data-item-id'],\n",
    "                        'parent_tweet_id': li['data-item-id'],\n",
    "                        'text': None,\n",
    "                        'user_id': div['data-user-id'],\n",
    "                        'user_screen_name': div['data-screen-name'],\n",
    "                        'user_name': div['data-name'],\n",
    "                        'retweets': 0,\n",
    "                        'likes': 0,\n",
    "                        'replies': 0,\n",
    "                        'timestamp': 0,\n",
    "                        'date': '',\n",
    "                        'reply': 1,\n",
    "                        'retweet': 0\n",
    "                    }\n",
    "\n",
    "                    # Tweet Text\n",
    "                    text_p_reply = div.find(\"p\", class_=\"tweet-text\")\n",
    "                    if text_p_reply is not None:\n",
    "                        reply['text'] = text_p_reply.get_text()\n",
    "\n",
    "                    # Tweet Retweets\n",
    "                    retweet_span = div.select(\n",
    "                        \"div.ProfileTweet-action--retweet span.ProfileTweet-actionCountForPresentation\")\n",
    "                    if retweet_span is not None and len(retweet_span) > 0 and retweet_span[0].get_text().strip() != '':\n",
    "                        reply['retweets'] = int(\n",
    "                            retweet_span[0].get_text())\n",
    "\n",
    "                    # Tweet Likes\n",
    "                    like_span = div.select(\n",
    "                        \"div.ProfileTweet-action--favorite span.ProfileTweet-actionCountForPresentation\")\n",
    "                    if like_span is not None and len(like_span) > 0 and like_span[0].get_text().strip() != '':\n",
    "                        reply['likes'] = int(\n",
    "                            like_span[0].get_text())\n",
    "\n",
    "                    # Tweet Replies\n",
    "                    reply_span = div.select(\n",
    "                        \"div.ProfileTweet-action--reply span.ProfileTweet-actionCountForPresentation\")\n",
    "                    if reply_span is not None and len(reply_span) > 0 and reply_span[0].get_text().strip() != '':\n",
    "                        reply['replies'] = int(\n",
    "                            reply_span[0].get_text())\n",
    "\n",
    "                    date_span = div.find(\"span\", class_=\"js-short-timestamp\")\n",
    "                    if date_span is not None:\n",
    "                        reply['timestamp'] = date_span['data-time']\n",
    "                        reply['date'] = datetime.utcfromtimestamp(\n",
    "                            int(reply['timestamp'])\n",
    "                        ).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                    tweets.append(reply)\n",
    "\n",
    "            try:\n",
    "                driver.execute_script(\n",
    "                    \"document.querySelector('.request-retweeted-popup').click()\")\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            time.sleep(3)\n",
    "\n",
    "            # extract the html for the whole lot:\n",
    "            page_source_retweets = driver.page_source\n",
    "\n",
    "            soup_retweets = bs(page_source_retweets, 'lxml')\n",
    "\n",
    "            for li2 in soup_retweets.find_all(attrs={\"data-item-type\": \"user\"}):\n",
    "                # If our li doesn't have a tweet-id, we skip it as it's not going to be a tweet.\n",
    "                if 'data-item-type' not in li2.attrs:\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    retweet = {\n",
    "                        'tweet_id': None,\n",
    "                        'parent_tweet_id': li['data-item-id'],\n",
    "                        'text': None,\n",
    "                        'user_id': None,\n",
    "                        'user_screen_name': None,\n",
    "                        'user_name': None,\n",
    "                        'retweets': 0,\n",
    "                        'likes': 0,\n",
    "                        'replies': 0,\n",
    "                        'timestamp': 0,\n",
    "                        'date': '',\n",
    "                        'reply': 0,\n",
    "                        'retweet': 1\n",
    "                    }\n",
    "\n",
    "                    # Tweet Text\n",
    "                    text_p_retweet = li2.find(\"p\", class_=\"bio\")\n",
    "                    if text_p_retweet is not None:\n",
    "                        retweet['text'] = text_p_retweet.get_text()\n",
    "\n",
    "                    # Tweet User ID, User Screen Name, User Name\n",
    "                    user_details_div_retweet = li2.find(\n",
    "                        \"div\", class_=\"account\")\n",
    "                    if user_details_div_retweet is not None:\n",
    "                        retweet['user_id'] = user_details_div_retweet['data-user-id']\n",
    "                        retweet['user_screen_name'] = user_details_div_retweet['data-screen-name']\n",
    "                        retweet['user_name'] = user_details_div_retweet['data-name']\n",
    "\n",
    "                    tweets.append(retweet)\n",
    "\n",
    "    return tweets\n",
    "\n",
    "\n",
    "def deduplicate_list_of_dicts(dct):\n",
    "    '''Remove multiple postings of one user as we only need 1 instance'''\n",
    "    user_list = []\n",
    "    for user in dct:\n",
    "        if user['user_screen_name'] not in user_list:\n",
    "            user_list.append(user['user_screen_name'])\n",
    "    return user_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Do not run this if you only want the user_following\n",
    "# # This will scrape the keyword, replace it with millionpeoplemarch, JanetNapoles or PDAFScam\n",
    "# # Replace email and password with your own\n",
    "# driver = init_driver()\n",
    "\n",
    "# login_twitter(driver, '<email>', '<password>')\n",
    "\n",
    "# hmtl = search_twitter(driver, '<keyword>')\n",
    "\n",
    "# tweets_mpm = extract_tweets(hmtl, driver)\n",
    "\n",
    "# close_driver(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:37:54.593530Z",
     "start_time": "2019-03-09T15:37:53.350059Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run this to get the distinct users from the tweets\n",
    "with open('data/tweets_merged.json', 'r') as infile:\n",
    "    tweets_mpm_from_file = json.load(infile)\n",
    "    user_list_mpm = deduplicate_list_of_dicts(tweets_mpm_from_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run this for FOLLOWERS not for FOLLOWING\n",
    "# # Run this only once ever as this only builds the tracker file\n",
    "# # If you run this again you will lose your progress tracker\n",
    "# with open('data/tweets_mpm_list.json', 'r') as outfile:\n",
    "#     json.dump(user_list_mpm, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T10:03:59.687464Z",
     "start_time": "2019-03-06T10:03:59.674367Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Run this to get FOLLOWERS so don't run if you want FOLLOWING users\n",
    "# with open('user_followers.json', 'r') as infile:\n",
    "#     user_followers = json.load(infile)\n",
    "# print(len(user_followers))\n",
    "    \n",
    "# with open('tweets_mpm_list.json', 'r') as infile:\n",
    "#     track_list = json.load(infile)\n",
    "# print(len(track_list))\n",
    "\n",
    "# for user in track_list:\n",
    "#     while True:\n",
    "#         try:\n",
    "#             driver = init_driver()\n",
    "#             login_twitter(driver, '<email>', '<password>')\n",
    "#             break\n",
    "#         except Exception:\n",
    "#             close_driver(driver)\n",
    "#             pass\n",
    "#     # open the web page in the browser:\n",
    "#     driver.get(\"https://twitter.com/\" +\n",
    "#                user + '/followers')\n",
    "#     followers_html = scrape_followers(driver)\n",
    "#     followers = extract_followers(followers_html)\n",
    "#     user_followers[user] = followers\n",
    "#     track_list = track_list[1:]\n",
    "#     with open('tweets_mpm_list.json', 'w') as outfile:\n",
    "#         json.dump(track_list, outfile)\n",
    "#     with open('user_followers.json', 'w') as outfile:\n",
    "#         json.dump(user_followers, outfile)\n",
    "#     close_driver(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T03:53:27.266561Z",
     "start_time": "2019-03-09T03:53:26.879663Z"
    }
   },
   "outputs": [],
   "source": [
    "# check if file exists\n",
    "try:\n",
    "    with open('data/user_following.json', 'r') as infile:\n",
    "        user_following = json.load(infile)\n",
    "except:\n",
    "    # create file if theres no file yet\n",
    "    f = open(\"data/user_following.json\", \"w+\")\n",
    "    f.write(\"{}\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-09T15:39:17.469Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8050\n",
      "1796\n"
     ]
    }
   ],
   "source": [
    "# Run this to get FOLLOWING so don't run if you want FOLLOWERS users\n",
    "# Run this for FOLLOWING not for FOLLOWERS\n",
    "\n",
    "# RUN TRACKER\n",
    "with open('data/user_following.json', 'r') as infile:\n",
    "    following_mpm_from_file = json.load(infile)\n",
    "    user_following_unique = list(set(following_mpm_from_file.keys()))\n",
    "    \n",
    "to_scrape = [user for user in user_list_mpm if user not in user_following_unique]\n",
    "\n",
    "with open('data/tweets_mpm_list_following.json', 'w') as outfile:\n",
    "    json.dump(to_scrape, outfile)\n",
    "    \n",
    "    \n",
    "# SCRAPER\n",
    "\n",
    "with open('data/user_following.json', 'r') as infile:\n",
    "        user_following = json.load(infile)\n",
    "\n",
    "print(len(set(user_following)))\n",
    "\n",
    "with open('data/tweets_mpm_list_following.json', 'r') as infile:\n",
    "    track_list = json.load(infile)\n",
    "print(len(track_list))\n",
    "\n",
    "driver = init_driver()\n",
    "login_twitter(driver, 'othepjavier@gmail.com', 'TheM@trix')\n",
    "\n",
    "for user in track_list:\n",
    "    #     while True:\n",
    "    #         try:\n",
    "    #             driver = init_driver()\n",
    "    #             login_twitter(driver, 'dybrian008@gmail.com', 'Imaginebreaker12')\n",
    "    #             break\n",
    "    #         except Exception:\n",
    "    #             close_driver(driver)\n",
    "    #             pass\n",
    "    # open the web page in the browser:\n",
    "    driver.get(\"https://twitter.com/\" +\n",
    "               user + '/following')\n",
    "    try:\n",
    "        following_html = scrape_followers(driver)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        following = extract_followers(following_html)\n",
    "    except Exception:\n",
    "        track_list = track_list[1:]\n",
    "        with open('data/tweets_mpm_list_following.json', 'w') as outfile:\n",
    "            json.dump(track_list, outfile)\n",
    "        continue\n",
    "    user_following[user] = following\n",
    "    track_list = track_list[1:]\n",
    "    with open('data/tweets_mpm_list_following.json', 'w') as outfile:\n",
    "        json.dump(track_list, outfile)\n",
    "    with open('data/user_following.json', 'w') as outfile:\n",
    "        json.dump(user_following, outfile)\n",
    "    # close_driver(driver)\n",
    "close_driver(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:38:01.883007Z",
     "start_time": "2019-03-09T15:38:01.872256Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9343"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_list_mpm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
